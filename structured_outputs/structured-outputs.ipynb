{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs\n",
    "**Structured Outputs** is a feature that ensures the model will always generate responses that adhere to your supplied Schema.\n",
    "Can be regex, Json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [](http://)List of concepts in this notebook:\n",
    "1. How LLM Understand the input\n",
    "2. What is Tokenizer\n",
    "3. How does LLM predict output text\n",
    "5. How to predict next possible characters for given regex that follow it\n",
    "6. How to restrict LLMs to generate only possible characters.\n",
    "7. But LLM is trained on tokens, how to convert next character possible to next token possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM is kind of a martix, that takes number as input and number as output, so we need to convert IO to numbers. But how can we do it?\n",
    "1. Convert each character to a number (0-9,A-B,a-b)\n",
    "2. Convert word to a number (Ice - 0, Cream - 1, Cone - 2)\n",
    "\n",
    "**Character-level tokenization**:\n",
    "1. Very long sequences (inefficient)\n",
    "2. Loses common patterns/subwords\n",
    "3. Model has to learn character combinations from scratch\n",
    "<br> Example: \"playing\" would be 7 tokens [p,l,a,y,i,n,g] instead of maybe 2 tokens [play, ing] (And computation can go input length^2 -> impractical)\n",
    "\n",
    "**Word-level tokenization**:\n",
    "1. Huge vocabulary size (millions of words)\n",
    "2. Can't handle unseen words (OOV problem)\n",
    "3. Wastes space on rare words\n",
    "4. No subword understanding\n",
    "<br> Example: If \"smartwatch\" isn't in vocabulary but \"smart\" and \"watch\" are known words, the model can't understand it (And First and Last layer will be huge matrices of length of vocabulary -> high computation)\n",
    "\n",
    "\n",
    "**Why BPE is Better**\n",
    "<br><br>\n",
    "**Adaptive Vocabulary**:\n",
    "* Starts with characters and iteratively merges most frequent pairs\n",
    "* Creates subword units that represent common patterns in the data\n",
    "* Can represent both frequent and rare words effectively\n",
    "<br><br>\n",
    "**Balance between character and word level**:\n",
    "* Common words stay as single tokens\n",
    "* Rare words split into meaningful subwords\n",
    "* Example: \"uncommon\" → [\"un\", \"common\"]\n",
    "<br><br>\n",
    "**Handles unseen words**:\n",
    "* Can break down new words into known subwords\n",
    "* Example: If model never saw \"teleporting\" but knows \"tele\" and \"porting\", it can still understand it\n",
    "<br><br>\n",
    "**Efficient sequence length**:\n",
    "* Much shorter than character-level\n",
    "* More informative than arbitrary splits\n",
    "* Example: \"playing\" might be [\"play\", \"ing\"] (2 tokens) instead of 7 characters\n",
    "<br><br>\n",
    "etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To check how it is done in python, please visit this page](https://huggingface.co/learn/nlp-course/en/chapter6/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:17.991399Z",
     "iopub.status.busy": "2025-02-08T15:34:17.990919Z",
     "iopub.status.idle": "2025-02-08T15:34:17.995132Z",
     "shell.execute_reply": "2025-02-08T15:34:17.994280Z",
     "shell.execute_reply.started": "2025-02-08T15:34:17.991362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install greenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:17.996931Z",
     "iopub.status.busy": "2025-02-08T15:34:17.996312Z",
     "iopub.status.idle": "2025-02-08T15:34:18.065845Z",
     "shell.execute_reply": "2025-02-08T15:34:18.064649Z",
     "shell.execute_reply.started": "2025-02-08T15:34:17.996790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:18.067166Z",
     "iopub.status.busy": "2025-02-08T15:34:18.066906Z",
     "iopub.status.idle": "2025-02-08T15:34:18.086041Z",
     "shell.execute_reply": "2025-02-08T15:34:18.084772Z",
     "shell.execute_reply.started": "2025-02-08T15:34:18.067145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:18.087365Z",
     "iopub.status.busy": "2025-02-08T15:34:18.087064Z",
     "iopub.status.idle": "2025-02-08T15:34:18.108447Z",
     "shell.execute_reply": "2025-02-08T15:34:18.107161Z",
     "shell.execute_reply.started": "2025-02-08T15:34:18.087339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-360M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:18.110070Z",
     "iopub.status.busy": "2025-02-08T15:34:18.109688Z",
     "iopub.status.idle": "2025-02-08T15:34:23.192984Z",
     "shell.execute_reply": "2025-02-08T15:34:23.191297Z",
     "shell.execute_reply.started": "2025-02-08T15:34:18.110036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32241ec3f4784071b81243d0795c5338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfa1ff9d06d4c68abd594dfd1bf8da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3637e26fed7a4c968a59b38389de104c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442098a69a3842c19381d33feac8657c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7199ad8657854f9ca9e5e1eb4971541c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:23.196271Z",
     "iopub.status.busy": "2025-02-08T15:34:23.195954Z",
     "iopub.status.idle": "2025-02-08T15:34:23.212577Z",
     "shell.execute_reply": "2025-02-08T15:34:23.211552Z",
     "shell.execute_reply.started": "2025-02-08T15:34:23.196208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26843, 665, 28, 638, 359, 346, 47]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Hi there, how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:23.214764Z",
     "iopub.status.busy": "2025-02-08T15:34:23.214451Z",
     "iopub.status.idle": "2025-02-08T15:34:23.231005Z",
     "shell.execute_reply": "2025-02-08T15:34:23.229907Z",
     "shell.execute_reply.started": "2025-02-08T15:34:23.214740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there, how are you?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([26843, 665, 28, 638, 359, 346, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:23.232618Z",
     "iopub.status.busy": "2025-02-08T15:34:23.232230Z",
     "iopub.status.idle": "2025-02-08T15:34:23.331310Z",
     "shell.execute_reply": "2025-02-08T15:34:23.330120Z",
     "shell.execute_reply.started": "2025-02-08T15:34:23.232582Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ically': 947,\n",
       " 'DEX': 31958,\n",
       " 'ricia': 30769,\n",
       " 'winning': 21664,\n",
       " 'ossibility': 46578,\n",
       " 'MES': 26826,\n",
       " 'bred': 27099,\n",
       " 'AW': 36025,\n",
       " 'relationship': 36861,\n",
       " 'requency': 12690,\n",
       " 'ĠHey': 19103,\n",
       " 'ĠF': 426,\n",
       " 'Ġfootnotes': 47746,\n",
       " 'St': 1393,\n",
       " 'recognized': 33850,\n",
       " 'Ġcalculating': 17085,\n",
       " 'inski': 36303,\n",
       " 'nder': 594,\n",
       " 'Ġgrabbing': 41105,\n",
       " 'burning': 34306,\n",
       " 'ĠShi': 23607,\n",
       " 'ĠEat': 16478,\n",
       " 'Ġromance': 18233,\n",
       " 'Ġnodules': 37543,\n",
       " 'ĠFiscal': 45161,\n",
       " 'Ġprodu': 796,\n",
       " 'Ġvigil': 15173,\n",
       " 'ĠAfric': 2079,\n",
       " 'ĠAlphabet': 34586,\n",
       " 'ĠMarch': 3903,\n",
       " 'Scientific': 29654,\n",
       " 'ĠRon': 17668,\n",
       " 'Ġallowing': 3910,\n",
       " 'Ġmenstruation': 31149,\n",
       " 'Ġzu': 47316,\n",
       " 'Enum': 21207,\n",
       " 'versions': 30511,\n",
       " 'ĠSilicon': 32688,\n",
       " 'ui': 9010,\n",
       " 'Ġbidding': 44746,\n",
       " 'Ġclump': 45949,\n",
       " 'đ': 205,\n",
       " 'Ġorganizer': 32704,\n",
       " 'Ġfurther': 2030,\n",
       " 'erence': 2095,\n",
       " 'Ġattractions': 21627,\n",
       " 'Ġinstitutions': 4679,\n",
       " 'variable': 18025,\n",
       " 'ĠBA': 27173,\n",
       " 'ĠOk': 16765,\n",
       " 'Ġhighest': 4919,\n",
       " 'Follow': 11101,\n",
       " 'Ġdrawback': 37762,\n",
       " 'ĠSyst': 46919,\n",
       " '>.': 19369,\n",
       " 'inqu': 20756,\n",
       " 'Ġaccording': 2289,\n",
       " 'Ġparaph': 27966,\n",
       " 'ĠWen': 42760,\n",
       " 'Ġ_(\"': 40410,\n",
       " 'Ġbelieve': 2875,\n",
       " '###': 3757,\n",
       " 'Gl': 10484,\n",
       " 'tym': 21778,\n",
       " 'ĠIntroducing': 30387,\n",
       " 'Ġcourses': 5836,\n",
       " 'Ġcerebral': 20260,\n",
       " 'related': 4235,\n",
       " 'ampling': 38577,\n",
       " 'ĠJ': 530,\n",
       " 'Ġfractions': 17161,\n",
       " 'Ġinstructional': 14883,\n",
       " 'ĠSuppose': 18644,\n",
       " 'Ġginger': 20303,\n",
       " 'Ġskin': 2574,\n",
       " 'Ġunexpected': 7039,\n",
       " 'ĠJazz': 31823,\n",
       " 'Ġcafeter': 40304,\n",
       " 'Ġexcellence': 19386,\n",
       " 'Ġidentify': 2669,\n",
       " 'Ġpatents': 20527,\n",
       " 'Estimated': 40158,\n",
       " 'Ġpandemics': 37765,\n",
       " 'Ġspecialist': 10828,\n",
       " 'ITES': 31612,\n",
       " 'Ġphysician': 9115,\n",
       " 'Ġresponsibility': 4974,\n",
       " 'umbs': 26934,\n",
       " 'iagn': 38508,\n",
       " 'ĠBulgarian': 39644,\n",
       " 'Ġescaping': 26634,\n",
       " 'Ġtenderness': 30190,\n",
       " 'approximately': 31178,\n",
       " 'Ġhelping': 4307,\n",
       " 'ĠNeurological': 46237,\n",
       " 'Ġpractice': 2180,\n",
       " 'oved': 2906,\n",
       " 'ĠStrip': 41529,\n",
       " 'Ġdisruptive': 25274,\n",
       " 'App': 6477,\n",
       " 'ĠVehicles': 35575,\n",
       " 'crapers': 39520,\n",
       " 'ĠPras': 48478,\n",
       " 'Ġdont': 35047,\n",
       " 'Ġelectoral': 20089,\n",
       " 'Ġventil': 38596,\n",
       " 'Ġinnoc': 21911,\n",
       " 'cd': 13133,\n",
       " 'Ġwraps': 34560,\n",
       " 'suggest': 35959,\n",
       " '{': 107,\n",
       " 'sequence': 19932,\n",
       " 'ĠInterpret': 35418,\n",
       " 'ĠRelative': 37295,\n",
       " 'ethod': 1186,\n",
       " 'acts': 6323,\n",
       " 'ĠBurn': 18204,\n",
       " 'erring': 8105,\n",
       " 'itual': 7912,\n",
       " 'Ġinfluences': 7843,\n",
       " 'Ġtoys': 9173,\n",
       " 'ĠPeterson': 34720,\n",
       " 'Ġwilliam': 45110,\n",
       " 'Ġblinded': 46193,\n",
       " 'Ġpenn': 23548,\n",
       " 'Ġtriggered': 14734,\n",
       " 'ĠCond': 11441,\n",
       " 'priority': 33176,\n",
       " 'Ġattachments': 26360,\n",
       " 'Ġparasites': 15178,\n",
       " 'Ġsettle': 5767,\n",
       " 'ĠTheodore': 26915,\n",
       " 'Ġcellphone': 44304,\n",
       " 'Ġflaps': 45967,\n",
       " 'Birds': 45510,\n",
       " 'ĠLicense': 6966,\n",
       " 'Ġwinds': 11113,\n",
       " 'Ġcryptocurrencies': 28448,\n",
       " 'Ġauthenticated': 48879,\n",
       " 'ÑĢÐ¸': 46879,\n",
       " 'Ġincrement': 28028,\n",
       " 'ĠTalking': 31450,\n",
       " 'ling': 1519,\n",
       " 'ĠDirectory': 28595,\n",
       " 'olar': 8784,\n",
       " 'ĠKush': 44396,\n",
       " 'Ġarbitrary': 18040,\n",
       " 'Ġdocumentary': 16262,\n",
       " 'Ġrecord': 2004,\n",
       " 'ĠScar': 31436,\n",
       " 'Ġpredicting': 18478,\n",
       " 'ecause': 3420,\n",
       " 'Ġroyalties': 47203,\n",
       " 'ĠQ': 1606,\n",
       " 'otide': 41843,\n",
       " 'Shape': 31266,\n",
       " 'izoph': 15217,\n",
       " '|': 108,\n",
       " 'Ġbelieved': 4484,\n",
       " 'Ġperspective': 4939,\n",
       " 'ĠLik': 36795,\n",
       " 'Ġretard': 26278,\n",
       " 'Ġamphib': 17764,\n",
       " 'Ġenduring': 12340,\n",
       " 'fx': 31392,\n",
       " 'ĠSet': 5427,\n",
       " 'Ġefficiency': 4890,\n",
       " 'ĠEn': 1698,\n",
       " 'Ġevergreen': 27103,\n",
       " 'ĠAg': 2850,\n",
       " 'CESS': 42452,\n",
       " 'GA': 14234,\n",
       " 'ĠDh': 18386,\n",
       " 'Ġinsulating': 32884,\n",
       " 'Ġno': 787,\n",
       " 'draw': 7254,\n",
       " 'ĠProviders': 46283,\n",
       " 'Ġepidem': 13296,\n",
       " 'Ġpickup': 40619,\n",
       " 'ĠWaterloo': 37875,\n",
       " 'happy': 45478,\n",
       " 'special': 19159,\n",
       " 'ĠCRT': 47281,\n",
       " 'ese': 2796,\n",
       " 'ĠProphet': 14303,\n",
       " 'Ġeast': 6251,\n",
       " 'ĠSiberian': 36614,\n",
       " 'Protocol': 24139,\n",
       " 'Ġabsorption': 10875,\n",
       " 'ĠAnother': 5839,\n",
       " 'rames': 16813,\n",
       " 'Ġgender': 4034,\n",
       " 'ĠGuide': 7870,\n",
       " 'Ġechoing': 39543,\n",
       " 'ERC': 31540,\n",
       " 'ĠPipe': 43551,\n",
       " 'ĠWeek': 11616,\n",
       " 'ĠTechnological': 28897,\n",
       " 'åı¯': 37139,\n",
       " 'Ġcollar': 25748,\n",
       " 'Ġrenewables': 27023,\n",
       " 'Ġsnapshots': 44407,\n",
       " 'Ġtubers': 41665,\n",
       " 'scientific': 29341,\n",
       " 'SERV': 41081,\n",
       " 'gathere': 38691,\n",
       " 'Male': 40716,\n",
       " 'store': 9887,\n",
       " 'çĲ': 41671,\n",
       " 'gen': 1639,\n",
       " 'istor': 36442,\n",
       " 'Ġeigenvalues': 43793,\n",
       " 'Ġfight': 3568,\n",
       " 'yaml': 23389,\n",
       " 'Ġkinds': 5479,\n",
       " 'Ġtwitter': 39453,\n",
       " 'Ġfascinated': 19049,\n",
       " 'Title': 16491,\n",
       " 'aram': 29066,\n",
       " 'DEFAULT': 23448,\n",
       " 'á¹Ľ': 43595,\n",
       " 'DESCRIPT': 42742,\n",
       " 'Ġimmig': 5525,\n",
       " 'Ġstandby': 48852,\n",
       " 'ĠCurtis': 35779,\n",
       " 'ĠLi': 9679,\n",
       " 'Ġ)': 2481,\n",
       " 'Ġrom': 7963,\n",
       " 'Ġbere': 39802,\n",
       " 'ĠSettings': 22806,\n",
       " 'Ġmedicines': 10037,\n",
       " 'Ġgraders': 23117,\n",
       " 'mot': 24264,\n",
       " 'gp': 29244,\n",
       " 'ynamics': 38614,\n",
       " 'ĠMarkov': 48713,\n",
       " 'guide': 30956,\n",
       " 'ĠConcerning': 47139,\n",
       " 'keras': 22548,\n",
       " 'Ġsid': 31262,\n",
       " 'fam': 11639,\n",
       " 'Ġswim': 7234,\n",
       " 'agul': 31989,\n",
       " 'Ġdelivers': 21959,\n",
       " 'Ġsummertime': 48949,\n",
       " 'horse': 32798,\n",
       " 'ĠBear': 19171,\n",
       " 'ĠRabbit': 33950,\n",
       " 'ĠNgu': 44943,\n",
       " 'Ġwiping': 40848,\n",
       " 'Ġhurried': 44489,\n",
       " 'ATION': 8985,\n",
       " 'Ir': 31302,\n",
       " 'Ġadhesive': 24660,\n",
       " 'controlled': 14912,\n",
       " \"...')\": 38958,\n",
       " 'ril': 21933,\n",
       " 'Ġally': 23119,\n",
       " 'USDA': 37729,\n",
       " 'Ġwidespread': 7123,\n",
       " 'fire': 10010,\n",
       " 'ĠBryant': 46289,\n",
       " 'iously': 10604,\n",
       " 'ĠIntroduce': 32630,\n",
       " 'Ġbroadcasts': 34925,\n",
       " 'ĠBut': 1249,\n",
       " 'ĠMadison': 19803,\n",
       " 'physics': 44541,\n",
       " 'Ġoverwhelm': 37928,\n",
       " 'osic': 46223,\n",
       " 'ĠPurple': 35608,\n",
       " 'bro': 11699,\n",
       " 'Ġpostures': 35877,\n",
       " 'Ġrecognition': 6303,\n",
       " 'Ġdeeply': 6592,\n",
       " 'Ġcritters': 32906,\n",
       " 'ĠReform': 19012,\n",
       " 'Ġdelivery': 7037,\n",
       " 'Ġindivid': 13287,\n",
       " 'Ġmedieval': 9649,\n",
       " 'Ġreass': 14185,\n",
       " 'thing': 1884,\n",
       " 'Ġhonorable': 41895,\n",
       " 'Ġpressures': 11820,\n",
       " 'Ġspecifying': 26149,\n",
       " 'Ġcompartment': 25901,\n",
       " 'Ġpoetic': 21174,\n",
       " 'Ġwear': 5777,\n",
       " 'ĠFabric': 35922,\n",
       " 'mentia': 10072,\n",
       " 'sec': 5016,\n",
       " 'Ġminer': 42462,\n",
       " 'Ġshore': 10393,\n",
       " 'conscious': 10475,\n",
       " ')!': 31050,\n",
       " 'ĠPartnership': 23657,\n",
       " 'Ġmodifiers': 48497,\n",
       " 'Ġpharmacist': 31578,\n",
       " 'raits': 16783,\n",
       " 'reason': 23759,\n",
       " 'Produ': 18082,\n",
       " 'gha': 47948,\n",
       " 'talk': 23482,\n",
       " 'Ġtopp': 41634,\n",
       " 'Population': 33107,\n",
       " 'los': 28584,\n",
       " 'ET': 3438,\n",
       " 'Ġtents': 32558,\n",
       " 'ĠHear': 33344,\n",
       " 'dump': 19488,\n",
       " 'Ġcommunicating': 13794,\n",
       " 'ĠProper': 16676,\n",
       " 'ĠLily': 14176,\n",
       " 'Ġmacrom': 48796,\n",
       " 'ara': 4075,\n",
       " 'vat': 44951,\n",
       " 'Ġcryptocur': 17550,\n",
       " 'Ġneuro': 5233,\n",
       " 'ĠAaron': 24694,\n",
       " 'days': 13680,\n",
       " 'Ġlof': 36522,\n",
       " 'Ġresolving': 21379,\n",
       " 'par': 1095,\n",
       " 'ĠBeng': 16295,\n",
       " 'ĠSuch': 5475,\n",
       " 'Ac': 10580,\n",
       " 'aird': 40118,\n",
       " 'Ġresponse': 2426,\n",
       " 'ĠBottom': 34828,\n",
       " 'ĠTrav': 42032,\n",
       " 'ĠACM': 39352,\n",
       " 'Ġcompeting': 11433,\n",
       " 'Ġfinger': 8400,\n",
       " 'Ġsettings': 4840,\n",
       " 'Ġvice': 12010,\n",
       " 'Ġvision': 4494,\n",
       " 'Ġvowels': 27763,\n",
       " 'ĠFa': 11881,\n",
       " 'Ġmont': 44753,\n",
       " 'ĠNormandy': 33009,\n",
       " 'urnal': 20798,\n",
       " 'Turn': 24202,\n",
       " 'ĠGrain': 43938,\n",
       " 'ĠJong': 47675,\n",
       " 'ĠPI': 21609,\n",
       " 'ĠPray': 20999,\n",
       " 'Ġfewer': 6694,\n",
       " 'movement': 42246,\n",
       " 'ĠEveryone': 14607,\n",
       " 'Ġroller': 22978,\n",
       " 'Ka': 44929,\n",
       " 'plasticity': 45443,\n",
       " 'âĢĶâĢĶ': 28101,\n",
       " 'Đ': 204,\n",
       " 'ĠPlus': 9933,\n",
       " 'Ġimpedance': 29774,\n",
       " 'Ġpublishes': 30892,\n",
       " 'Ġepidemic': 12912,\n",
       " 'ĠAndrew': 10795,\n",
       " 'Equ': 17298,\n",
       " 'Ġimprint': 30312,\n",
       " 'Ġovershadowed': 47159,\n",
       " 'bearing': 18885,\n",
       " 'Ġneurology': 46528,\n",
       " 'Ġtrademark': 24486,\n",
       " 'ĠNor': 6334,\n",
       " 'Ġbiopsy': 20525,\n",
       " 'Ġstandardized': 13361,\n",
       " 'Ġstature': 33396,\n",
       " 'Ġtrails': 16475,\n",
       " 'Ġvibrant': 8685,\n",
       " 'Ġaneurysm': 45247,\n",
       " 'ĠBever': 32693,\n",
       " 'amond': 47670,\n",
       " 'ortium': 20631,\n",
       " 'ĠRebell': 28915,\n",
       " 'Ġcolonize': 43610,\n",
       " 'exclude': 35744,\n",
       " 'fo': 30138,\n",
       " 'Ġfaire': 33315,\n",
       " 'qs': 33733,\n",
       " 'ĠGregorian': 42612,\n",
       " 'Ġdemographic': 15212,\n",
       " 'inplace': 46033,\n",
       " 'Ġinsurers': 39155,\n",
       " 'ĠGnostic': 47243,\n",
       " 'Ġenthusiasm': 13836,\n",
       " 'ĠJah': 41829,\n",
       " 'Ġki': 42535,\n",
       " 'Ġplaster': 26683,\n",
       " 'angi': 27796,\n",
       " 'Ġdesired': 6253,\n",
       " 'Ġvulnerability': 13047,\n",
       " 'çļ': 11484,\n",
       " 'Ġorganisation': 13082,\n",
       " 'ĠEmpress': 43022,\n",
       " 'ĠFrancesco': 45155,\n",
       " 'Group': 13352,\n",
       " 'ĠTil': 39135,\n",
       " 'Ġavoids': 28766,\n",
       " 'Ġbraking': 30435,\n",
       " 'Ġlack': 3096,\n",
       " 'Ġmacros': 47876,\n",
       " \"!'\": 34379,\n",
       " 'mens': 46060,\n",
       " 'Ġgems': 22582,\n",
       " 'Ïİ': 26569,\n",
       " 'Ġlegacy': 9009,\n",
       " 'Ġpodcasts': 29534,\n",
       " 'Ġdash': 20740,\n",
       " 'Ġexceeded': 22260,\n",
       " 'Cos': 43385,\n",
       " 'Ġinnocuous': 45550,\n",
       " 'Ġwoven': 17942,\n",
       " 'Ġboundless': 35789,\n",
       " 'cluster': 14893,\n",
       " 'ĠArmen': 13953,\n",
       " 'Ġchicken': 9610,\n",
       " 'Ġshark': 17521,\n",
       " 'ĠkW': 36398,\n",
       " 'Ġsuit': 9066,\n",
       " 'scatter': 25978,\n",
       " 'umping': 26141,\n",
       " 'unciation': 13777,\n",
       " 'ĠFalls': 19911,\n",
       " 'ĠRatio': 34378,\n",
       " 'Ġhous': 3932,\n",
       " 'ĠKeynes': 40887,\n",
       " 'Ġheaters': 28301,\n",
       " 'Ġscattering': 24484,\n",
       " 'xuality': 24187,\n",
       " 'origin': 25129,\n",
       " 'rolysis': 45689,\n",
       " 'roph': 5041,\n",
       " 'ĠTrack': 26960,\n",
       " 'ĠPerl': 39862,\n",
       " 'Ġprotect': 2037,\n",
       " 'ttps': 3023,\n",
       " 'ĠAunt': 44864,\n",
       " 'ĠViet': 8131,\n",
       " 'reviewed': 21200,\n",
       " 'Ne': 9042,\n",
       " 'ĠChristie': 47568,\n",
       " 'Ġflooded': 23642,\n",
       " 'ĠBowl': 33926,\n",
       " 'hearted': 20461,\n",
       " 'Developing': 35675,\n",
       " 'Ġcocaine': 22207,\n",
       " 'Ġgib': 38133,\n",
       " 'ĠPri': 31029,\n",
       " 'Ġaggravate': 45492,\n",
       " 'Ġentering': 9245,\n",
       " 'Ġhone': 24585,\n",
       " 'Ġtransplants': 29568,\n",
       " 'Ġamphibian': 42131,\n",
       " 'inis': 17182,\n",
       " 'ĠHels': 39951,\n",
       " 'inoza': 46466,\n",
       " 'kv': 42124,\n",
       " 'ĠClayton': 48288,\n",
       " 'Ġneighbors': 10645,\n",
       " 'price': 17517,\n",
       " 'Ġwished': 20190,\n",
       " 'ĠAmeric': 1052,\n",
       " 'Ġresemblance': 29348,\n",
       " 'square': 20170,\n",
       " 'poke': 44894,\n",
       " 'Ġaesthetics': 16648,\n",
       " 'Ġflakes': 34149,\n",
       " 'Ġappreciating': 19669,\n",
       " 'Instead': 18640,\n",
       " 'aid': 3755,\n",
       " 'à¹': 31933,\n",
       " '/,': 35869,\n",
       " 'ĠMalt': 39662,\n",
       " 'ĠIU': 19635,\n",
       " 'Ġcontainers': 10367,\n",
       " 'Ġdedu': 29071,\n",
       " 'multip': 32168,\n",
       " 'Ġdevoid': 26486,\n",
       " 'Ġfarther': 16928,\n",
       " 'Ġconfidentiality': 24467,\n",
       " 'Ġdrying': 15193,\n",
       " 'asian': 19977,\n",
       " 'Ġaggression': 15901,\n",
       " 'ĠWeimar': 47056,\n",
       " 'Ġawarded': 12090,\n",
       " 'Ġintegrate': 12131,\n",
       " 'ĠLe': 2250,\n",
       " 'Ġmythological': 29375,\n",
       " 'An': 2089,\n",
       " '¦Ĥ': 43580,\n",
       " 'Ġremot': 18123,\n",
       " 'ĠHiggs': 38669,\n",
       " 'ĠLl': 22883,\n",
       " 'Ġshout': 19266,\n",
       " 'mund': 21319,\n",
       " 'color': 6713,\n",
       " 'ĠBCE': 15023,\n",
       " 'ĠBaal': 47918,\n",
       " 'Ġaffirm': 12741,\n",
       " 'urls': 20936,\n",
       " 'Ġbends': 37331,\n",
       " 'Ġdatas': 43876,\n",
       " 'Ġdeterminants': 26625,\n",
       " 'Ġpil': 6451,\n",
       " 'Ġwet': 5793,\n",
       " 'Ġunless': 6365,\n",
       " 'beans': 24365,\n",
       " 'zh': 26308,\n",
       " 'Ġmashed': 42572,\n",
       " 'Ġmodifier': 44618,\n",
       " 'nton': 39602,\n",
       " 'arthy': 29437,\n",
       " 'bat': 11325,\n",
       " 'Ġcapillary': 35339,\n",
       " 'Ġmembership': 13405,\n",
       " 'Ġcoping': 13036,\n",
       " 'ernel': 8558,\n",
       " 'ĠMinimal': 48749,\n",
       " 'Ġstopped': 8976,\n",
       " 'Ġar': 562,\n",
       " 'ĠCastro': 31192,\n",
       " 'Ġailments': 23394,\n",
       " 'ogs': 9891,\n",
       " 'gins': 34724,\n",
       " 'DS': 8521,\n",
       " 'ermis': 32770,\n",
       " 'ĠDrum': 37011,\n",
       " 'ĠFarming': 30868,\n",
       " 'ĠAstronomy': 24840,\n",
       " 'ĠMerced': 45334,\n",
       " 'ĠSometimes': 5938,\n",
       " 'ĠPlains': 21155,\n",
       " 'ĠWyoming': 20720,\n",
       " 'Ġbelongings': 28834,\n",
       " 'Ġcolder': 21434,\n",
       " 'Ġcultures': 4520,\n",
       " 'Ġfigs': 38702,\n",
       " 'brow': 24719,\n",
       " 'amp': 1613,\n",
       " 'ĠHighlands': 39826,\n",
       " 'Ġfacet': 25485,\n",
       " 'Ġobstruct': 28450,\n",
       " 'haven': 45240,\n",
       " 'Ġble': 6673,\n",
       " 'Ġcucumbers': 35701,\n",
       " 'Ġmultipl': 11292,\n",
       " 'Ġtactics': 12330,\n",
       " 'ĠComponents': 26812,\n",
       " 'ĠMunicipal': 35505,\n",
       " 'Ġscant': 38008,\n",
       " 'Ġmarble': 18540,\n",
       " 'Ġworsh': 21701,\n",
       " 'Ġprimate': 34881,\n",
       " 'Ġrecomm': 2377,\n",
       " 'ajor': 1618,\n",
       " 'Ġcushion': 31040,\n",
       " 'Dep': 11254,\n",
       " 'Ġdecisions': 3884,\n",
       " 'imon': 31719,\n",
       " 'Ġspine': 11465,\n",
       " 'lit': 17094,\n",
       " 'ptic': 8851,\n",
       " 'adh': 29694,\n",
       " 'ĠSpelling': 39116,\n",
       " 'Ġferment': 30107,\n",
       " 'Ġbalancing': 14412,\n",
       " 'Ġclearly': 4763,\n",
       " 'ĠBuch': 28246,\n",
       " 'ĠDougl': 13483,\n",
       " 'Ġweary': 42778,\n",
       " 'Ġ$': 1885,\n",
       " 'Ġcurrencies': 24219,\n",
       " 'Ġspecializes': 26719,\n",
       " 'Ġthermostat': 35062,\n",
       " 'ALT': 43878,\n",
       " 'Ġcrocodile': 40728,\n",
       " 'purple': 47078,\n",
       " 'Ġfishermen': 24133,\n",
       " 'Ig': 44311,\n",
       " 'Ġbyproduct': 39677,\n",
       " 'reponame': 10368,\n",
       " 'ĠNottingham': 42307,\n",
       " 'mand': 25759,\n",
       " 'ĠDecisions': 36871,\n",
       " 'Ġattest': 37102,\n",
       " 'Ġinterpreter': 28269,\n",
       " 'ĠBronx': 45777,\n",
       " 'ĠCONDIT': 25991,\n",
       " 'ĠSensor': 35632,\n",
       " 'upon': 25705,\n",
       " 'Ġdef': 753,\n",
       " 'Ġparticipant': 15634,\n",
       " 'Ġran': 7674,\n",
       " 'Ġyoung': 1805,\n",
       " 'Ġasserted': 26405,\n",
       " 'Parameter': 21018,\n",
       " 'ĠColoring': 42954,\n",
       " 'Ġdeaths': 6517,\n",
       " \"'d\": 6737,\n",
       " 'Ġ))': 28192,\n",
       " 'ĠDemocrat': 32397,\n",
       " 'ĠSanct': 30852,\n",
       " 'Ġdeadline': 19652,\n",
       " 'Ġaphids': 30735,\n",
       " 'Ġgauges': 40649,\n",
       " 'ĠRoll': 21338,\n",
       " 'overs': 13337,\n",
       " 'Social': 15210,\n",
       " 'prev': 23243,\n",
       " 'ING': 4728,\n",
       " 'Ġfairly': 7699,\n",
       " 'Ġrarer': 46773,\n",
       " 'Ġregulation': 7943,\n",
       " 'Ter': 25425,\n",
       " 'RP': 29482,\n",
       " 'Ġriparian': 33869,\n",
       " 'etch': 7668,\n",
       " 'Ġdoctrinal': 43888,\n",
       " 'hydro': 43947,\n",
       " 'Ġsys': 7602,\n",
       " 'Ġthou': 17072,\n",
       " '_)': 30157,\n",
       " 'ĠSomething': 23368,\n",
       " 'Ġrural': 5947,\n",
       " 'Ġacreage': 40713,\n",
       " 'Ġcomposers': 24734,\n",
       " 'omerase': 48690,\n",
       " 'Ġbehaving': 35049,\n",
       " 'Ġpossessions': 21074,\n",
       " 'Ġgravel': 19282,\n",
       " 'Ġdeduct': 30766,\n",
       " 'Learning': 14433,\n",
       " 'english': 40019,\n",
       " 'Ġinfantry': 20681,\n",
       " 'Normal': 20828,\n",
       " 'Explore': 24869,\n",
       " 'Ġmasculine': 28366,\n",
       " 'Comm': 9482,\n",
       " 'ĠCO': 3214,\n",
       " 'Ġedible': 17748,\n",
       " 'Ġnex': 40076,\n",
       " 'Ġparticulate': 26528,\n",
       " 'Ġfind': 1042,\n",
       " 'usch': 25618,\n",
       " 'Side': 31310,\n",
       " 'DES': 47983,\n",
       " 'ĠEthiopia': 17640,\n",
       " 'ighter': 6827,\n",
       " 'Ġrefineries': 48134,\n",
       " 'widgets': 46102,\n",
       " 'Ġstacks': 33291,\n",
       " 'ava': 6056,\n",
       " 'dp': 24901,\n",
       " 'calls': 34723,\n",
       " 'ĠChey': 42827,\n",
       " 'utÃ©': 38501,\n",
       " 'ĠMacedonia': 34415,\n",
       " 'licts': 28463,\n",
       " 'Ġsesame': 36773,\n",
       " 'gue': 34652,\n",
       " 'ĠGate': 18447,\n",
       " 'Ġlyric': 43068,\n",
       " 'OCs': 28745,\n",
       " 'Split': 38251,\n",
       " ')|': 9756,\n",
       " 'ĠNative': 6647,\n",
       " 'ame': 513,\n",
       " 'ĠCypri': 44991,\n",
       " 'ĠPatagon': 48127,\n",
       " 'ĠSalad': 44990,\n",
       " 'Ġskulls': 37496,\n",
       " 'pronounced': 27587,\n",
       " 'Ġcumin': 43444,\n",
       " '-------': 14337,\n",
       " 'Ġfavorites': 29514,\n",
       " 'Ġsporadic': 36877,\n",
       " 'Ġprove': 6824,\n",
       " 'Mes': 45642,\n",
       " 'Interface': 21079,\n",
       " 'Ġexploratory': 32376,\n",
       " 'ĠTaking': 14985,\n",
       " '(\"{}': 43331,\n",
       " 'Ġspecific': 1678,\n",
       " 'ivar': 38062,\n",
       " 'Ġunconditional': 38383,\n",
       " '({': 10713,\n",
       " 'Ġarsen': 15888,\n",
       " 'Ġtactic': 31595,\n",
       " 'ĠHyd': 11486,\n",
       " 'Static': 46868,\n",
       " 'alls': 3255,\n",
       " 'ĠId': 13061,\n",
       " 'aedia': 44533,\n",
       " 'ĠSab': 12984,\n",
       " 'Ġattitude': 9459,\n",
       " 'ourag': 10881,\n",
       " 'Certain': 25577,\n",
       " 'Sch': 17291,\n",
       " '=%': 23446,\n",
       " 'access': 9465,\n",
       " 'ĠSmithsonian': 22175,\n",
       " 'Ġhybrids': 24705,\n",
       " 'Ġbitcoin': 31735,\n",
       " 'Ġsweeteners': 29603,\n",
       " 'Ġunwilling': 24731,\n",
       " 'Msg': 43841,\n",
       " 'Ġregional': 5533,\n",
       " 'ĠART': 28145,\n",
       " 'Ġpolymorph': 28032,\n",
       " 'African': 25078,\n",
       " 'Ġeruption': 18374,\n",
       " 'Ġwin': 4726,\n",
       " 'Ġdeep': 2276,\n",
       " 'Ġprospective': 16633,\n",
       " 'Ġacet': 19762,\n",
       " 'Ġsurgical': 10133,\n",
       " 'ĠHousing': 22023,\n",
       " 'ruct': 961,\n",
       " 'Ġremodeling': 44432,\n",
       " 'Ġtownships': 48211,\n",
       " 'Ġtrustees': 41333,\n",
       " 'db': 4868,\n",
       " 'magnitude': 47863,\n",
       " 'ĠCoron': 24530,\n",
       " 'ĠActa': 40484,\n",
       " 'Ġlabelled': 30039,\n",
       " 'subnet': 46468,\n",
       " 'For': 2193,\n",
       " 'Paris': 38634,\n",
       " 'ĠOrange': 19923,\n",
       " 'ĠSunshine': 49027,\n",
       " 'ĠTransgender': 47818,\n",
       " 'Ġarchive': 17428,\n",
       " 'Ġsilhouette': 47901,\n",
       " 'istant': 9531,\n",
       " 'Ġquota': 33249,\n",
       " 'charging': 40576,\n",
       " 'Ġbankruptcy': 27724,\n",
       " 'Ġdefending': 19325,\n",
       " 'ĠStreng': 18828,\n",
       " 'Ġcompile': 24162,\n",
       " 'Ġheroin': 28126,\n",
       " 'Ġpedagogy': 28391,\n",
       " 'Ġprotections': 17104,\n",
       " 'Ġdiagnose': 12794,\n",
       " 'Ġkids': 3081,\n",
       " 'Ġtransmitted': 10307,\n",
       " 'Ġrefrigerant': 40769,\n",
       " 'Ġsimilar': 1887,\n",
       " 'Ġnervosa': 40648,\n",
       " 'Ġresurrect': 39471,\n",
       " 'Ġtax': 3024,\n",
       " 'blocking': 46136,\n",
       " 'ĠDEC': 36128,\n",
       " 'Ġexposition': 33392,\n",
       " 'Ġhob': 11127,\n",
       " 'Ġoccasional': 15373,\n",
       " 'yset': 29530,\n",
       " 'ĠRating': 39988,\n",
       " 'power': 8857,\n",
       " 'Ġcontain': 2127,\n",
       " 'Ġputs': 10779,\n",
       " 'Ġpublicity': 30066,\n",
       " 'Ġstrings': 12345,\n",
       " 'Ġbiodiversity': 9201,\n",
       " 'Ġpear': 17306,\n",
       " 'ĠðŁĻĤ': 47526,\n",
       " 'ezers': 47776,\n",
       " 'ĠBerm': 31185,\n",
       " 'itates': 45694,\n",
       " 'ĠFrankenstein': 43290,\n",
       " 'Ġanticipate': 19071,\n",
       " 'Ġcandid': 5462,\n",
       " 'assan': 26491,\n",
       " 'Ġessence': 9615,\n",
       " 'Ġinsists': 32904,\n",
       " 'Ġqu': 613,\n",
       " 'Ġstruck': 10856,\n",
       " 'ĠPyTorch': 43943,\n",
       " 'Ġinfancy': 24126,\n",
       " 'Ġdepicted': 13178,\n",
       " 'Ġneph': 27878,\n",
       " 'inth': 12749,\n",
       " 'ĠChina': 3275,\n",
       " 'ĠSteel': 21088,\n",
       " 'ĠSPF': 38546,\n",
       " 'Ġchampion': 21033,\n",
       " 'Ġsuffixes': 44329,\n",
       " 'ĠProcessing': 18784,\n",
       " 'Ġcampfire': 48541,\n",
       " 'ĠNorman': 19255,\n",
       " 'Ġmetres': 13897,\n",
       " 'Ġfield': 1955,\n",
       " 'gr': 9861,\n",
       " 'Ġindicators': 11376,\n",
       " 'Ġdoll': 5661,\n",
       " 'ĠAlfred': 18587,\n",
       " 'ĠRalph': 25214,\n",
       " 'Ġramp': 17194,\n",
       " 'Ġanalyzing': 8218,\n",
       " 'Ġfreelance': 36636,\n",
       " 'Ġdoing': 2567,\n",
       " 'Ġmarine': 6562,\n",
       " 'Ġdolph': 16341,\n",
       " 'Ġexplicitly': 11075,\n",
       " 'divisions': 40888,\n",
       " 'endswith': 27061,\n",
       " 'uilding': 5385,\n",
       " 'ĠHorse': 19949,\n",
       " 'ĠJun': 12355,\n",
       " 'ĠPutting': 32181,\n",
       " 'ĠGregory': 21473,\n",
       " 'Ġdisadvantages': 17106,\n",
       " 'Ġenhance': 4695,\n",
       " 'ĊĊĠĠĠ': 1004,\n",
       " 'Ġreclaim': 26400,\n",
       " 'Ġxi': 41626,\n",
       " 'ĠAK': 29819,\n",
       " 'ĠAllison': 44716,\n",
       " 'ĠMedicines': 42828,\n",
       " 'Ġendeavor': 20640,\n",
       " 'Exc': 32223,\n",
       " 'ĠDistricts': 46776,\n",
       " 'Ġintervals': 12752,\n",
       " 'Div': 21757,\n",
       " 'ĠWinn': 30831,\n",
       " 'Ġditches': 43388,\n",
       " 'Ġrehabilit': 35627,\n",
       " 'olesc': 7946,\n",
       " 'pping': 5350,\n",
       " 'ĠAster': 36277,\n",
       " 'ĠStorm': 22706,\n",
       " 'Ġfacilitators': 48479,\n",
       " 'Ġsperm': 13283,\n",
       " 'tips': 49061,\n",
       " 'ĠConfig': 17399,\n",
       " 'ĠAmph': 33026,\n",
       " 'Ġexchange': 5487,\n",
       " 'Ġoriginates': 26091,\n",
       " 'ĠSlide': 35748,\n",
       " 'Ġstatute': 24414,\n",
       " 'rection': 8676,\n",
       " 'Ġadjectives': 27729,\n",
       " 'Ġdispers': 11747,\n",
       " 'agn': 1480,\n",
       " 'ĠJoe': 16632,\n",
       " 'Ġextant': 28306,\n",
       " 'ß': 170,\n",
       " 'ĠBarcelona': 30010,\n",
       " 'Ġdisciplines': 10578,\n",
       " 'Ġideation': 48774,\n",
       " 'Ġcapt': 2670,\n",
       " 'ĠAB': 17683,\n",
       " 'Ġkwargs': 12244,\n",
       " 'ĠSurgical': 35252,\n",
       " 'Ġhelicopters': 33967,\n",
       " 'Ġrebellious': 39303,\n",
       " 'Ġsurprises': 25025,\n",
       " 'ĠSac': 13330,\n",
       " 'mber': 2604,\n",
       " 'ung': 2280,\n",
       " 'aco': 22667,\n",
       " 'gged': 11723,\n",
       " 'Ġelemental': 34285,\n",
       " 'Ġiss': 1309,\n",
       " 'Ġemail': 4106,\n",
       " 'Ġtravel': 2827,\n",
       " 'ĠKer': 22632,\n",
       " 'Ġdyeing': 46858,\n",
       " 'Percent': 37653,\n",
       " 'uler': 15698,\n",
       " 'izo': 47810,\n",
       " 'Submitted': 45136,\n",
       " 'Attr': 41441,\n",
       " 'inous': 28368,\n",
       " 'lining': 26512,\n",
       " 'Ġinteract': 2298,\n",
       " 'Ġmarriages': 21336,\n",
       " 'ij': 5271,\n",
       " 'Ġatonement': 46264,\n",
       " 'country': 16664,\n",
       " '][': 4885,\n",
       " 'hed': 770,\n",
       " 'Ġcompens': 8062,\n",
       " 'drained': 36849,\n",
       " 'ĠMyth': 20192,\n",
       " 'ĠCong': 4279,\n",
       " 'ĠYOU': 12246,\n",
       " 'Ġpencils': 25487,\n",
       " 'Ġelevated': 10960,\n",
       " 'ĠSmall': 9946,\n",
       " 'Ġwatermelon': 38482,\n",
       " 'Ġornaments': 33809,\n",
       " 'ĠGeoffrey': 37909,\n",
       " 'ĠMay': 2405,\n",
       " 'akra': 31674,\n",
       " 'lett': 24263,\n",
       " 'fre': 29415,\n",
       " 'ĠProduct': 10550,\n",
       " 'Ġeukary': 29804,\n",
       " 'itness': 5197,\n",
       " 'Ġlaugh': 13731,\n",
       " 'Ġsuccessor': 19729,\n",
       " 'ĠRoosevelt': 13763,\n",
       " 'ĠNice': 44558,\n",
       " 'Ġdevelops': 10627,\n",
       " 'ĠBotany': 45442,\n",
       " 'reads': 44364,\n",
       " 'cod': 24733,\n",
       " 'ĠHumor': 47155,\n",
       " 'ĠHazard': 30629,\n",
       " 'Ġadmirable': 43500,\n",
       " 'community': 29707,\n",
       " 'bidden': 40219,\n",
       " 'Ġ/>': 37516,\n",
       " '©': 119,\n",
       " 'Ġangina': 48205,\n",
       " 'Ġanimals': 2355,\n",
       " 'entials': 13051,\n",
       " 'ĠThat': 1848,\n",
       " 'Ġdeveloping': 2941,\n",
       " 'Ġlunches': 43122,\n",
       " 'Ġorgan': 1192,\n",
       " 'Ġê': 35792,\n",
       " 'Ġconsensus': 12286,\n",
       " 'password': 12333,\n",
       " 'Ġse': 427,\n",
       " 'dard': 34510,\n",
       " 'Ġmonsoon': 30351,\n",
       " 'Ġscreens': 13128,\n",
       " 'Ġcomment': 5189,\n",
       " 'Ġpowered': 13147,\n",
       " 'ighth': 11935,\n",
       " 'ophilus': 38870,\n",
       " 'Ġremaining': 6219,\n",
       " 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ': 36489,\n",
       " 'Flex': 45226,\n",
       " 'essim': 31434,\n",
       " 'ĠNL': 46808,\n",
       " 'Ġcane': 25299,\n",
       " 'Ġdecreed': 48727,\n",
       " 'Ġmagically': 48917,\n",
       " 'Ġsecreted': 33868,\n",
       " 'Ġsoar': 43253,\n",
       " 'ĠMF': 46982,\n",
       " 'Play': 20522,\n",
       " 'Ġunhappy': 25107,\n",
       " 'Ġwillingness': 17870,\n",
       " 'Ġexisted': 9557,\n",
       " 'ized': 1005,\n",
       " 'Ġelectro': 7836,\n",
       " 'Ġstraightforward': 14857,\n",
       " 'Ġmagnitude': 10684,\n",
       " 'Ġali': 42219,\n",
       " 'ĠBasin': 16463,\n",
       " 'čĊčĊč': 16980,\n",
       " 'ĠFamilies': 19466,\n",
       " 'Ġconced': 31482,\n",
       " 'unction': 3682,\n",
       " 'Ġsuite': 21383,\n",
       " 'RT': 16895,\n",
       " 'everything': 44105,\n",
       " 'raph': 1258,\n",
       " 'ĠBy': 1428,\n",
       " 'ĠHearing': 23392,\n",
       " 'ĠDul': 38732,\n",
       " 'FC': 12236,\n",
       " 'ĠHolistic': 45543,\n",
       " 'Ġcent': 996,\n",
       " 'edies': 12854,\n",
       " 'Ġneurotransmitter': 31027,\n",
       " 'Ġprovince': 10416,\n",
       " 'Ġthrombosis': 42800,\n",
       " 'Ġtilt': 27067,\n",
       " 'ĠMonroe': 30662,\n",
       " 'Ġalgebraic': 26850,\n",
       " 'Ġarchaic': 36610,\n",
       " 'Ġpopping': 33122,\n",
       " 'HET': 44879,\n",
       " '||-': 35974,\n",
       " 'ĠLocated': 30517,\n",
       " 'issa': 21936,\n",
       " 'determination': 21440,\n",
       " 'æı': 46980,\n",
       " 'Ġdt': 21807,\n",
       " 'Ġsplend': 23415,\n",
       " 'Ġtotaling': 48725,\n",
       " 'Ġbelow': 2441,\n",
       " 'Ġfibrosis': 26183,\n",
       " 'omnia': 17358,\n",
       " 'Ġjournalists': 18242,\n",
       " 'ĠJuvenile': 22779,\n",
       " 'éĽ': 44846,\n",
       " 'ĠSter': 27520,\n",
       " 'orig': 7320,\n",
       " 'ĠMontgomery': 23671,\n",
       " 'Ġaffinity': 24523,\n",
       " 'aters': 4361,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:23.332582Z",
     "iopub.status.busy": "2025-02-08T15:34:23.332247Z",
     "iopub.status.idle": "2025-02-08T15:34:23.352160Z",
     "shell.execute_reply": "2025-02-08T15:34:23.350750Z",
     "shell.execute_reply.started": "2025-02-08T15:34:23.332546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:23.353578Z",
     "iopub.status.busy": "2025-02-08T15:34:23.353218Z",
     "iopub.status.idle": "2025-02-08T15:34:23.376050Z",
     "shell.execute_reply": "2025-02-08T15:34:23.374906Z",
     "shell.execute_reply.started": "2025-02-08T15:34:23.353551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "id_to_token = {token_id: token for token,token_id in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:23.377852Z",
     "iopub.status.busy": "2025-02-08T15:34:23.377342Z",
     "iopub.status.idle": "2025-02-08T15:34:23.399513Z",
     "shell.execute_reply": "2025-02-08T15:34:23.398450Z",
     "shell.execute_reply.started": "2025-02-08T15:34:23.377811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "Ġthere\n",
      ",\n",
      "Ġhow\n",
      "Ġare\n",
      "Ġyou\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for i in [26843, 665, 28, 638, 359, 346, 47]:\n",
    "    print(id_to_token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Outputs ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:23.401353Z",
     "iopub.status.busy": "2025-02-08T15:34:23.400808Z",
     "iopub.status.idle": "2025-02-08T15:34:44.922756Z",
     "shell.execute_reply": "2025-02-08T15:34:44.921551Z",
     "shell.execute_reply.started": "2025-02-08T15:34:23.401315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737f71a9088649bda64facdf5c7a2a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5135d3cb95c342afb77d7917ad65d6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b1e6e7a25a4f13bae5227b3c2018cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:44.924951Z",
     "iopub.status.busy": "2025-02-08T15:34:44.924591Z",
     "iopub.status.idle": "2025-02-08T15:34:44.931061Z",
     "shell.execute_reply": "2025-02-08T15:34:44.929840Z",
     "shell.execute_reply.started": "2025-02-08T15:34:44.924923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_next_token_predictions(model, input_ids, top_k=10):\n",
    "    \"\"\"\n",
    "    Get top k predictions for the next token with their probabilities\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        input_ids: Input token ids (tensor)\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (token_id, probability)\n",
    "    \"\"\"\n",
    "    # Get model's raw output (logits)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids) # Passing the input ids to the models\n",
    "        logits = outputs.logits # models spits out probabilty of each token\n",
    "\n",
    "    # Get the last token's predictions\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # Get top k probabilities and corresponding token ids\n",
    "    top_k_probs, top_k_tokens = torch.topk(next_token_probs, top_k)\n",
    "    \n",
    "    # Convert to list of tuples (token_id, probability)\n",
    "    predictions = [\n",
    "        (token.item(), prob.item()) \n",
    "        for token, prob in zip(top_k_tokens, top_k_probs)\n",
    "    ]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:44.932280Z",
     "iopub.status.busy": "2025-02-08T15:34:44.931971Z",
     "iopub.status.idle": "2025-02-08T15:34:45.249915Z",
     "shell.execute_reply": "2025-02-08T15:34:45.249002Z",
     "shell.execute_reply.started": "2025-02-08T15:34:44.932254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ' you', Probability: 0.9479\n",
      "Token: ' things', Probability: 0.0180\n",
      "Token: ' we', Probability: 0.0064\n",
      "Token: ' your', Probability: 0.0053\n",
      "Token: ' the', Probability: 0.0042\n",
      "Token: ' u', Probability: 0.0027\n",
      "Token: ' ya', Probability: 0.0022\n",
      "Token: ' y', Probability: 0.0015\n",
      "Token: ' ye', Probability: 0.0015\n",
      "Token: ' ', Probability: 0.0011\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, how are\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Get predictions\n",
    "predictions = get_next_token_predictions(model, input_ids)\n",
    "\n",
    "# Print results with decoded tokens\n",
    "for token_id, prob in predictions:\n",
    "    token_text = tokenizer.decode(token_id)\n",
    "    print(f\"Token: '{token_text}', Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see what is the output of the llm after one prediction, LLM spits one token at a time, and then pass our input+output token as input again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Concept alert): Sampling of output tokens, Not needed for this, but a small topic on how do we sample predicted outputs of LLM to have best output sequence\n",
    "1. Greedy search\n",
    "2. Exhaustive search and\n",
    "3. Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:45.251112Z",
     "iopub.status.busy": "2025-02-08T15:34:45.250846Z",
     "iopub.status.idle": "2025-02-08T15:34:45.485602Z",
     "shell.execute_reply": "2025-02-08T15:34:45.484352Z",
     "shell.execute_reply.started": "2025-02-08T15:34:45.251082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: '4', Probability: 0.8685\n",
      "Token: '2', Probability: 0.0483\n",
      "Token: '5', Probability: 0.0366\n",
      "Token: '3', Probability: 0.0151\n",
      "Token: '6', Probability: 0.0102\n",
      "Token: '1', Probability: 0.0090\n",
      "Token: '0', Probability: 0.0073\n",
      "Token: '7', Probability: 0.0017\n",
      "Token: '8', Probability: 0.0012\n",
      "Token: '9', Probability: 0.0006\n"
     ]
    }
   ],
   "source": [
    "input_text = 'What is the value of 2+2? The answer is '\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Get predictions\n",
    "predictions = get_next_token_predictions(model, input_ids)\n",
    "\n",
    "# Print results with decoded tokens\n",
    "for token_id, prob in predictions:\n",
    "    token_text = tokenizer.decode(token_id)\n",
    "    print(f\"Token: '{token_text}', Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A complete output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:34:45.486902Z",
     "iopub.status.busy": "2025-02-08T15:34:45.486560Z",
     "iopub.status.idle": "2025-02-08T15:35:02.155256Z",
     "shell.execute_reply": "2025-02-08T15:35:02.154289Z",
     "shell.execute_reply.started": "2025-02-08T15:34:45.486874Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in a small town named Harmonyville, there lived a group of animals who loved to learn about the world around them. One day, they noticed something strange - their little home was getting hotter every day because of a big storm that had hit nearby. So"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.7, stop_token=\"<|endoftext|>\"):\n",
    "    \"\"\"\n",
    "    Generate text continuation from a prompt until stop token or max length.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: Initial text prompt\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        stop_token: Token to stop generation\n",
    "        \n",
    "    Returns:\n",
    "        Generated text including the prompt\n",
    "    \"\"\"\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    generated_text = prompt\n",
    "    \n",
    "    # Generate tokens until stop condition\n",
    "    for _ in range(max_length):\n",
    "        # Get model's output for current sequence\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            next_token_logits = next_token_logits / temperature # What is the temperature we use in the prompt?\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Decode token\n",
    "            next_token_text = tokenizer.decode(next_token)\n",
    "            print(next_token_text, end=\"\", flush=True)\n",
    "            \n",
    "            # Check for stop condition\n",
    "            if stop_token in next_token_text:\n",
    "                break\n",
    "                \n",
    "            # Append to generated text\n",
    "            generated_text += next_token_text\n",
    "            \n",
    "            # Append token to input_ids for next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"Once upon a time\"\n",
    "generated = generate_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "# print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are these we use in the openai prompt? Temperature, top_k and top_p?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:35:02.156598Z",
     "iopub.status.busy": "2025-02-08T15:35:02.156250Z",
     "iopub.status.idle": "2025-02-08T15:35:02.160470Z",
     "shell.execute_reply": "2025-02-08T15:35:02.159338Z",
     "shell.execute_reply.started": "2025-02-08T15:35:02.156574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Apply top-k filtering\n",
    "# if top_k > 0:\n",
    "#     indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "#     next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "# Apply top-p filtering\n",
    "# if top_p < 1.0:\n",
    "#     sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "#     cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "#     sorted_indices_to_remove = cumulative_probs > top_p\n",
    "#     sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "#     sorted_indices_to_remove[..., 0] = 0\n",
    "#     indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "#     next_token_logits[indices_to_remove] = float('-inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Inutition on forcing structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:35:02.164838Z",
     "iopub.status.busy": "2025-02-08T15:35:02.164535Z",
     "iopub.status.idle": "2025-02-08T15:35:03.579953Z",
     "shell.execute_reply": "2025-02-08T15:35:03.578924Z",
     "shell.execute_reply.started": "2025-02-08T15:35:02.164812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: '4', Probability: 0.8585\n",
      "Token: '2', Probability: 0.0494\n",
      "Token: '5', Probability: 0.0351\n",
      "Token: '1', Probability: 0.0170\n",
      "Token: '6', Probability: 0.0114\n",
      "Token: '0', Probability: 0.0114\n",
      "Token: '3', Probability: 0.0110\n",
      "Token: '7', Probability: 0.0022\n",
      "Token: '8', Probability: 0.0018\n",
      "Token: '9', Probability: 0.0008\n"
     ]
    }
   ],
   "source": [
    "input_text = 'What is the value of 2+2? ANS: {\"answer\": '\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Get predictions\n",
    "predictions = get_next_token_predictions(model, input_ids)\n",
    "\n",
    "# Print results with decoded tokens\n",
    "for token_id, prob in predictions:\n",
    "    token_text = tokenizer.decode(token_id)\n",
    "    print(f\"Token: '{token_text}', Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:35:03.581418Z",
     "iopub.status.busy": "2025-02-08T15:35:03.581135Z",
     "iopub.status.idle": "2025-02-08T15:35:03.853630Z",
     "shell.execute_reply": "2025-02-08T15:35:03.852549Z",
     "shell.execute_reply.started": "2025-02-08T15:35:03.581395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ',', Probability: 0.3861\n",
      "Token: '}', Probability: 0.3794\n",
      "Token: ',\"', Probability: 0.0607\n",
      "Token: '}}', Probability: 0.0550\n",
      "Token: '},', Probability: 0.0404\n",
      "Token: '.', Probability: 0.0122\n",
      "Token: ' }', Probability: 0.0116\n",
      "Token: '}\"', Probability: 0.0067\n",
      "Token: '};', Probability: 0.0042\n",
      "Token: '}.', Probability: 0.0040\n"
     ]
    }
   ],
   "source": [
    "input_text = 'What is the value of 2+2? ANS: {\"answer\": 4'\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Get predictions\n",
    "predictions = get_next_token_predictions(model, input_ids)\n",
    "\n",
    "# Print results with decoded tokens\n",
    "for token_id, prob in predictions:\n",
    "    token_text = tokenizer.decode(token_id)\n",
    "    print(f\"Token: '{token_text}', Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:35:03.855281Z",
     "iopub.status.busy": "2025-02-08T15:35:03.854848Z",
     "iopub.status.idle": "2025-02-08T15:35:07.153661Z",
     "shell.execute_reply": "2025-02-08T15:35:07.152428Z",
     "shell.execute_reply.started": "2025-02-08T15:35:03.855244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Format: Result = X.\n",
      "Generated: Result = 4.\n",
      "\n",
      "Format: Answer:X\n",
      "Generated: Answer:2\n",
      "\n",
      "Format: {\"answer\": X}\n",
      "Generated: {\"answer\": 4}\n"
     ]
    }
   ],
   "source": [
    "def generate_formatted_answer(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    question, \n",
    "    format_prefix,\n",
    "    format_suffix,\n",
    "    allowed_tokens=None\n",
    "):\n",
    "    # Tokenize prefix and suffix\n",
    "    prefix_tokens = tokenizer.encode(format_prefix, add_special_tokens=False)\n",
    "    suffix_tokens = tokenizer.encode(format_suffix, add_special_tokens=False)\n",
    "    \n",
    "    # If no allowed tokens specified, use digits\n",
    "    if allowed_tokens is None:\n",
    "        allowed_tokens = tokenizer.encode(\"0123456789\", add_special_tokens=False)\n",
    "    \n",
    "    # Create input\n",
    "    input_ids = tokenizer(question, return_tensors=\"pt\").input_ids\n",
    "    generated = []\n",
    "    \n",
    "    # Generate prefix\n",
    "    for expected_token in prefix_tokens:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Force the expected token\n",
    "            mask = torch.full_like(next_token_logits, float('-inf'))\n",
    "            mask[expected_token] = 0\n",
    "            next_token_logits += mask\n",
    "            \n",
    "            next_token = torch.argmax(next_token_logits).unsqueeze(0)\n",
    "            generated.append(next_token.item())\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    # Generate answer (allow only specified tokens)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        mask = torch.full_like(next_token_logits, float('-inf'))\n",
    "        mask[allowed_tokens] = 0\n",
    "        next_token_logits += mask\n",
    "        \n",
    "        next_token = torch.argmax(next_token_logits).unsqueeze(0)\n",
    "        generated.append(next_token.item())\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "    # Generate suffix\n",
    "    for expected_token in suffix_tokens:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            mask = torch.full_like(next_token_logits, float('-inf'))\n",
    "            mask[expected_token] = 0\n",
    "            next_token_logits += mask\n",
    "            \n",
    "            next_token = torch.argmax(next_token_logits).unsqueeze(0)\n",
    "            generated.append(next_token.item())\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Example usage:\n",
    "formats = [\n",
    "    ('Result = ', '.'),\n",
    "    ('Answer:', ''),\n",
    "    ('{\"answer\": ', '}')\n",
    "]\n",
    "\n",
    "question = \"What is the aswer of 2+2?, The answer is\"\n",
    "for prefix, suffix in formats:\n",
    "    result = generate_formatted_answer(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        question,\n",
    "        format_prefix=prefix,\n",
    "        format_suffix=suffix\n",
    "    )\n",
    "    print(f\"\\nFormat: {prefix}X{suffix}\")\n",
    "    print(f\"Generated: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It has a higher chance of performance detoriation, as we are disturbing the actual probabilty distribution, So, recent LLMs that offer structured outputs are trained to see lot of JSONs, to make accurate answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuctured Outputs, REGEX -> How to find next possible character?\n",
    "You can build a FSM (Finite State Machine) to validate state for the input you are given and check next possible valid states.\n",
    "Not going through this here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But wait LLM's are not character predictors!!! So how do make next possible characters predictor to next possible tokens predictor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BruteForce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:35:13.849016Z",
     "iopub.status.busy": "2025-02-08T15:35:13.848733Z",
     "iopub.status.idle": "2025-02-08T15:35:13.853826Z",
     "shell.execute_reply": "2025-02-08T15:35:13.852949Z",
     "shell.execute_reply.started": "2025-02-08T15:35:13.848992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_preds(predictions):\n",
    "    for token_id in predictions[:10]:\n",
    "        token_text = tokenizer.decode(token_id)\n",
    "        print(f\"Token: '{token_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:39:10.381266Z",
     "iopub.status.busy": "2025-02-08T15:39:10.380940Z",
     "iopub.status.idle": "2025-02-08T15:39:10.397823Z",
     "shell.execute_reply": "2025-02-08T15:39:10.396660Z",
     "shell.execute_reply.started": "2025-02-08T15:39:10.381242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: '{\"answer\":', Token: '1', Valid: True\n",
      "Current: '{\"answer\":1', Token: '2', Valid: True\n",
      "Current: '{\"answer\":12', Token: '3', Valid: True\n",
      "Current: '{\"answer\":123', Token: '}', Valid: True\n",
      "Current: '', Token: '{', Valid: True\n"
     ]
    }
   ],
   "source": [
    "class SimpleRegexFSM:\n",
    "    def __init__(self, pattern_type=\"json_number\"):\n",
    "        self.pattern_type = pattern_type\n",
    "        self.states = {\n",
    "            'start': {'{': 'open_brace'},\n",
    "            'open_brace': {'\"': 'quote1'},\n",
    "            'quote1': {'a': 'a'},\n",
    "            'a': {'n': 'n'},\n",
    "            'n': {'s': 's'},\n",
    "            's': {'w': 'w'},\n",
    "            'w': {'e': 'e'},\n",
    "            'e': {'r': 'r'},\n",
    "            'r': {'\"': 'quote2'},\n",
    "            'quote2': {':': 'colon'},\n",
    "            'colon': {'0':'number', '1':'number', '2':'number', '3':'number', \n",
    "                     '4':'number', '5':'number', '6':'number', '7':'number', \n",
    "                     '8':'number', '9':'number'},\n",
    "            'number': {'}':'end', '0':'number', '1':'number', '2':'number', \n",
    "                      '3':'number', '4':'number', '5':'number', '6':'number', \n",
    "                      '7':'number', '8':'number', '9':'number'},\n",
    "            'end': {}\n",
    "        }\n",
    "        self.accept_states = {'end'}\n",
    "        \n",
    "    def is_valid_continuation(self, current_str, token):\n",
    "        \"\"\"Check if adding token maintains valid path\"\"\"\n",
    "        test_str = current_str + token\n",
    "        \n",
    "        # Track current state\n",
    "        state = 'start'\n",
    "        for char in test_str:\n",
    "            # Get valid transitions from current state\n",
    "            valid_transitions = self.states.get(state, {})\n",
    "            \n",
    "            # Check if this character is valid\n",
    "            if char not in valid_transitions:\n",
    "                return False\n",
    "                \n",
    "            # Move to next state\n",
    "            state = valid_transitions[char]\n",
    "            \n",
    "        # Current state should either be accepting or have valid transitions\n",
    "        return state in self.accept_states or bool(self.states.get(state, {}))\n",
    "\n",
    "def generate_constrained_text(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt, \n",
    "    max_length=50, \n",
    "    temperature=0.7,\n",
    "    top_k=10,\n",
    "    debug=False\n",
    "):\n",
    "    decoder = SimpleRegexFSM()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    current_str = \"\"  # Start with empty string for regex matching\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Get top-k predictions\n",
    "            top_k_probs, top_k_indices = torch.topk(\n",
    "                F.softmax(next_token_logits, dim=-1),\n",
    "                k=len(vocab)\n",
    "            )\n",
    "            \n",
    "            # Check which tokens maintain valid regex\n",
    "            valid_mask = torch.zeros_like(top_k_probs)\n",
    "            for i, token_id in enumerate(top_k_indices):\n",
    "                # Convert tensor to integer for decoding\n",
    "                token_text = tokenizer.decode([token_id.item()], skip_special_tokens=True)\n",
    "                if decoder.is_valid_continuation(current_str, token_text):\n",
    "                    valid_mask[i] = 1\n",
    "            \n",
    "            # Mask invalid tokens\n",
    "            masked_probs = top_k_probs * valid_mask\n",
    "            if masked_probs.sum() == 0:\n",
    "                if debug:\n",
    "                    print(\"No valid tokens found\")\n",
    "                break\n",
    "                \n",
    "            # Sample next token\n",
    "            masked_probs = masked_probs / masked_probs.sum()\n",
    "            chosen_idx = torch.multinomial(masked_probs, num_samples=1)\n",
    "            next_token_id = top_k_indices[chosen_idx].item()\n",
    "\n",
    "            if debug:\n",
    "                print(\"###\",next_token_id)\n",
    "            # Update state\n",
    "            next_token_text = tokenizer.decode([next_token_id], skip_special_tokens=True)\n",
    "            current_str += next_token_text\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]])], dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Current string: {current_str}\")\n",
    "            \n",
    "            # Check if complete\n",
    "            if current_str and current_str[-1] == '}':\n",
    "                break\n",
    "    \n",
    "    return current_str\n",
    "\n",
    "# Test the implementation\n",
    "fsm = SimpleRegexFSM()\n",
    "test_cases = [\n",
    "    ('{\"answer\":', '1'),\n",
    "    ('{\"answer\":1', '2'),\n",
    "    ('{\"answer\":12', '3'),\n",
    "    ('{\"answer\":123', '}'),\n",
    "    ('', '{'),\n",
    "]\n",
    "\n",
    "for current, token in test_cases:\n",
    "    valid = fsm.is_valid_continuation(current, token)\n",
    "    print(f\"Current: '{current}', Token: '{token}', Valid: {valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:39:10.993422Z",
     "iopub.status.busy": "2025-02-08T15:39:10.993029Z",
     "iopub.status.idle": "2025-02-08T15:39:29.463829Z",
     "shell.execute_reply": "2025-02-08T15:39:29.462336Z",
     "shell.execute_reply.started": "2025-02-08T15:39:10.993394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 0\n",
      "Current string: \n",
      "### 107\n",
      "Current string: {\n",
      "### 18\n",
      "Current string: {\"\n",
      "### 81\n",
      "Current string: {\"a\n",
      "### 0\n",
      "Current string: {\"a\n",
      "### 94\n",
      "Current string: {\"an\n",
      "### 13356\n",
      "Current string: {\"answer\n",
      "### 18\n",
      "Current string: {\"answer\"\n",
      "### 0\n",
      "Current string: {\"answer\"\n",
      "### 42\n",
      "Current string: {\"answer\":\n",
      "### 35\n",
      "Current string: {\"answer\":3\n",
      "### 33\n",
      "Current string: {\"answer\":31\n",
      "### 34\n",
      "Current string: {\"answer\":312\n",
      "### 37\n",
      "Current string: {\"answer\":3125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8b6821074990>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_constrained_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"What is the answer of 2+2?, Answer it in json format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-e75405b62d4f>\u001b[0m in \u001b[0;36mgenerate_constrained_text\u001b[0;34m(model, tokenizer, prompt, max_length, temperature, top_k, debug)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# Convert tensor to integer for decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mtoken_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_valid_continuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mvalid_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3841\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         clean_up_tokenization_spaces = (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_constrained_text(model, tokenizer, \"What is the answer of 2+2?, Answer it in json format\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:36:29.229315Z",
     "iopub.status.busy": "2025-02-08T15:36:29.228927Z",
     "iopub.status.idle": "2025-02-08T15:36:29.244150Z",
     "shell.execute_reply": "2025-02-08T15:36:29.242594Z",
     "shell.execute_reply.started": "2025-02-08T15:36:29.229277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing FSM with example cases:\n",
      "Current: '<', Token: '\\n', Valid: True (expected True)\n",
      "Current: '<\\n', Token: 'text', Valid: True (expected True)\n",
      "Current: '<\\ntext', Token: '|', Valid: True (expected True)\n",
      "Current: '<\\ntext|', Token: '42', Valid: True (expected True)\n",
      "Current: '<\\ntext|42', Token: '|', Valid: True (expected True)\n",
      "Current: '<\\ntext|42|', Token: 'more', Valid: True (expected True)\n",
      "Current: '<\\ntext|42|more', Token: '\\n', Valid: True (expected True)\n",
      "Current: '<\\ntext|42|more\\ntext|13|text', Token: '>', Valid: True (expected True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TableFormatFSM:\n",
    "    def __init__(self):\n",
    "        # Define character sets:\n",
    "        self.LETTERS = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "        self.NUMBERS = set(\"0123456789\")\n",
    "    \n",
    "    def step(self, state, char):\n",
    "        # Returns the next state given the current state and an input character,\n",
    "        # or None if the char is not allowed.\n",
    "\n",
    "        if state == 'start':\n",
    "            if char == '<':\n",
    "                return 'after_open'\n",
    "            return None\n",
    "\n",
    "        elif state == 'after_open':\n",
    "            if char == '\\n':\n",
    "                return 'row_start'\n",
    "            return None\n",
    "\n",
    "        elif state == 'row_start':\n",
    "            # At the beginning of a row, our first field (text) must start with a letter.\n",
    "            if char in self.LETTERS:\n",
    "                return 'text1'\n",
    "            return None\n",
    "\n",
    "        elif state == 'text1':\n",
    "            # In field1 (text): we can accept more letters or a pipe indicating the end of field1.\n",
    "            if char in self.LETTERS:\n",
    "                return 'text1'  # Remain in text1\n",
    "            if char == '|':\n",
    "                return 'after_pipe1'\n",
    "            return None\n",
    "\n",
    "        elif state == 'after_pipe1':\n",
    "            # Field2 must start with a digit.\n",
    "            if char in self.NUMBERS:\n",
    "                return 'number'\n",
    "            return None\n",
    "\n",
    "        elif state == 'number':\n",
    "            # In field2 (number): continue to accept digits or a pipe to end the number.\n",
    "            if char in self.NUMBERS:\n",
    "                return 'number'\n",
    "            if char == '|':\n",
    "                return 'after_pipe2'\n",
    "            return None\n",
    "\n",
    "        elif state == 'after_pipe2':\n",
    "            # Field3 (text) must start with a letter.\n",
    "            if char in self.LETTERS:\n",
    "                return 'text2'\n",
    "            return None\n",
    "\n",
    "        elif state == 'text2':\n",
    "            # In field3 (text): allow letters; then either newline (for a new row) or closing '>' to finish.\n",
    "            if char in self.LETTERS:\n",
    "                return 'text2'\n",
    "            if char == '\\n':\n",
    "                return 'row_start'\n",
    "            if char == '>':\n",
    "                return 'end'\n",
    "            return None\n",
    "\n",
    "        elif state == 'end':\n",
    "            # Once ended, nothing more is accepted.\n",
    "            return None\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_state(self, text):\n",
    "        \"\"\"\n",
    "        Process the entire string `text` from the beginning and return the last reached state.\n",
    "        If at any point an invalid character is encountered, return None.\n",
    "        \"\"\"\n",
    "        state = 'start'\n",
    "        for char in text:\n",
    "            next_state = self.step(state, char)\n",
    "            if next_state is None:\n",
    "                return None\n",
    "            state = next_state\n",
    "        return state\n",
    "    \n",
    "    def is_valid_continuation(self, current_str, token):\n",
    "        \"\"\"\n",
    "        Check if appending token to current_str is a valid partial continuation.\n",
    "        (It does not require that the whole table is complete—only that the FSM does not\n",
    "        get stuck.)\n",
    "        \"\"\"\n",
    "        state = self.get_state(current_str)\n",
    "        if state is None:\n",
    "            return False\n",
    "        for char in token:\n",
    "            state = self.step(state, char)\n",
    "            if state is None:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def is_accepting(self, text):\n",
    "        \"\"\"\n",
    "        We consider the string accepted only if the FSM is in state 'end'\n",
    "        (i.e. the table is complete).\n",
    "        \"\"\"\n",
    "        return self.get_state(text) == 'end'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the FSM:\n",
    "    fsm = TableFormatFSM()\n",
    "    test_cases = [\n",
    "        # (current string, token to add, expected valid?)\n",
    "        ('<', '\\n', True),\n",
    "        ('<\\n', 'text', True),\n",
    "        ('<\\ntext', '|', True),\n",
    "        ('<\\ntext|', '42', True),\n",
    "        ('<\\ntext|42', '|', True),\n",
    "        ('<\\ntext|42|', 'more', True),\n",
    "        ('<\\ntext|42|more', '\\n', True),\n",
    "        ('<\\ntext|42|more\\ntext|13|text', '>', True),\n",
    "    ]\n",
    "\n",
    "    print(\"Testing FSM with example cases:\")\n",
    "    for current, token, expected in test_cases:\n",
    "        valid = fsm.is_valid_continuation(current, token)\n",
    "        print(f\"Current: {repr(current)}, Token: {repr(token)}, Valid: {valid} (expected {expected})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:41:27.999765Z",
     "iopub.status.busy": "2025-02-08T15:41:27.999333Z",
     "iopub.status.idle": "2025-02-08T15:41:28.008948Z",
     "shell.execute_reply": "2025-02-08T15:41:28.007736Z",
     "shell.execute_reply.started": "2025-02-08T15:41:27.999733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_constrained_text(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt, \n",
    "    max_length=50, \n",
    "    temperature=0.7,\n",
    "    top_k=10,\n",
    "    debug=False\n",
    "):\n",
    "    decoder = TableFormatFSM()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    current_str = \"\"  # Start with empty string for regex matching\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Get top-k predictions\n",
    "            top_k_probs, top_k_indices = torch.topk(\n",
    "                F.softmax(next_token_logits, dim=-1),\n",
    "                k=len(vocab)\n",
    "            )\n",
    "            \n",
    "            # Check which tokens maintain valid regex\n",
    "            valid_mask = torch.zeros_like(top_k_probs)\n",
    "            for i, token_id in enumerate(top_k_indices):\n",
    "                # Convert tensor to integer for decoding\n",
    "                token_text = tokenizer.decode([token_id.item()], skip_special_tokens=True)\n",
    "                if decoder.is_valid_continuation(current_str, token_text):\n",
    "                    valid_mask[i] = 1\n",
    "            \n",
    "            # Mask invalid tokens\n",
    "            masked_probs = top_k_probs * valid_mask\n",
    "            if masked_probs.sum() == 0:\n",
    "                if debug:\n",
    "                    print(\"No valid tokens found\")\n",
    "                break\n",
    "                \n",
    "            # Sample next token\n",
    "            masked_probs = masked_probs / masked_probs.sum()\n",
    "            chosen_idx = torch.multinomial(masked_probs, num_samples=1)\n",
    "            next_token_id = top_k_indices[chosen_idx].item()\n",
    "\n",
    "            if debug:\n",
    "                print(\"###\",next_token_id)\n",
    "            # Update state\n",
    "            next_token_text = tokenizer.decode([next_token_id], skip_special_tokens=True)\n",
    "            current_str += next_token_text\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]])], dim=1)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Current string: {current_str}\")\n",
    "            \n",
    "            # Check if complete\n",
    "            if current_str and current_str[-1] == '}':\n",
    "                break\n",
    "    \n",
    "    return current_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:44:49.518207Z",
     "iopub.status.busy": "2025-02-08T15:44:49.517527Z",
     "iopub.status.idle": "2025-02-08T15:45:29.414214Z",
     "shell.execute_reply": "2025-02-08T15:45:29.412582Z",
     "shell.execute_reply.started": "2025-02-08T15:44:49.518159Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0|>|<\n",
      "0|>|<\n",
      "0|>|<\n",
      "\n",
      "Explanation:\n",
      "\n",
      "Raju has 3 apples and 1 banana,\n",
      "\n",
      "Geeta has 1 banana and 2 tomato.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"There are 3 people Raju, Geeta and Bhanu, Raju has 3 apples, Geeta has 1 banana and Bhanu has 2 tomato. Tell the information about raju, geeta anf bhanu and fruitfs they have in format <\\nperson|count|type\\n>\"\"\"\n",
    "generated = generate_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=50,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T15:47:49.014704Z",
     "iopub.status.busy": "2025-02-08T15:47:49.014281Z",
     "iopub.status.idle": "2025-02-08T15:48:49.971821Z",
     "shell.execute_reply": "2025-02-08T15:48:49.970625Z",
     "shell.execute_reply.started": "2025-02-08T15:47:49.014673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 44\n",
      "Current string: <\n",
      "### 198\n",
      "Current string: <\n",
      "\n",
      "### 66\n",
      "Current string: <\n",
      "R\n",
      "### 1346\n",
      "Current string: <\n",
      "Raj\n",
      "### 101\n",
      "Current string: <\n",
      "Raju\n",
      "### 108\n",
      "Current string: <\n",
      "Raju|\n",
      "### 35\n",
      "Current string: <\n",
      "Raju|3\n",
      "### 108\n",
      "Current string: <\n",
      "Raju|3|\n",
      "### 48773\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "### 198\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "\n",
      "### 9488\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Ge\n",
      "### 8810\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta\n",
      "### 108\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|\n",
      "### 33\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1\n",
      "### 108\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|\n",
      "### 2947\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|ban\n",
      "### 3231\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "### 198\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "\n",
      "### 50\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "B\n",
      "### 10936\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhan\n",
      "### 101\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhanu\n",
      "### 108\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhanu|\n",
      "### 34\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhanu|2\n",
      "### 108\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhanu|2|\n",
      "### 39753\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhanu|2|tom\n",
      "### 6401\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhanu|2|tomato\n",
      "### 198\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhanu|2|tomato\n",
      "\n",
      "### 20230\n",
      "Current string: <\n",
      "Raju|3|apples\n",
      "Geeta|1|banana\n",
      "Bhanu|2|tomato\n",
      "Print\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ecc057794c9d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_constrained_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\"There are 3 people Raju, Geeta and Bhanu, Raju has 3 apples, Geeta has 1 banana and Bhanu has 2 tomato. Tell the information about raju, geeta anf bhanu and fruits they have in format <\\nperson|count|type\\n>\"\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-505b927b97c7>\u001b[0m in \u001b[0;36mgenerate_constrained_text\u001b[0;34m(model, tokenizer, prompt, max_length, temperature, top_k, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Convert tensor to integer for decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mtoken_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_valid_continuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mvalid_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3427ac1fccbc>\u001b[0m in \u001b[0;36mis_valid_continuation\u001b[0;34m(self, current_str, token)\u001b[0m\n\u001b[1;32m     91\u001b[0m         get stuck.)\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3427ac1fccbc>\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_constrained_text(model, tokenizer, \"\"\"There are 3 people Raju, Geeta and Bhanu, Raju has 3 apples, Geeta has 1 banana and Bhanu has 2 tomato. Tell the information about raju, geeta anf bhanu and fruits they have in format <\\nperson|count|type\\n>\"\"\", debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is not generic for a given regex, I only used a custom structure to show how it can be done. \n",
    "To actually check how to do it, Please check the paper \n",
    "\n",
    "https://arxiv.org/html/2407.08103v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
